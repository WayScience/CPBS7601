{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep-Dive Scientific Analysis Template\n",
        "\n",
        "This notebook is a step-by-step, **research-ready** template for performing a deep dive analysis on a dataset.\n",
        "\n",
        "It emphasizes:\n",
        "- Reproducibility (seed control, environment capture, deterministic ops)\n",
        "- Exploratory data analysis (EDA) and data quality checks\n",
        "- Feature engineering and leakage-aware modeling\n",
        "- Robust validation (cross-validation, bootstrap CIs)\n",
        "- Explainability (permutation importance, partial dependence)\n",
        "- Statistical inference (hypothesis tests, effect sizes)\n",
        "- Artifact capture (figures, tables, model, and environment)\n",
        "\n",
        "> Tip: Duplicate this notebook per project and keep it under version control (Git).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup & Reproducibility\n",
        "\n",
        "Configure libraries, paths, and random seeds. Decide whether to load a real dataset\n",
        "or synthesize one for demonstration/reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os, sys, math, json, random, pathlib\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # REQUIRED: use matplotlib (no seaborn)\n",
        "from typing import Tuple\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Paths\n",
        "PROJECT_DIR = Path.cwd()\n",
        "ARTIFACTS = PROJECT_DIR / \"artifacts\"\n",
        "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project directory: {PROJECT_DIR}\")\n",
        "print(f\"Artifacts directory: {ARTIFACTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Edit `DATA_PATH` to point at your CSV/Parquet file. If `DATA_PATH` is `None`,\n",
        "we'll synthesize a toy dataset (binary outcome + mixed features) so all steps run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your dataset path here (CSV). Example: Path('data/mydata.csv')\n",
        "DATA_PATH = None  # Replace with a Path(\"data/your_file.csv\") or keep None to synthesize.\n",
        "\n",
        "if DATA_PATH is not None and Path(DATA_PATH).exists():\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "else:\n",
        "    # Synthesize a realistic tabular dataset\n",
        "    n = 1200\n",
        "    X1 = np.random.normal(0, 1, n)\n",
        "    X2 = np.random.normal(5, 2, n)\n",
        "    X3 = np.random.binomial(1, 0.3, n)\n",
        "    X4 = np.random.uniform(-2, 4, n)\n",
        "    # Nonlinear outcome + noise for binary classification\n",
        "    logits = 0.5*X1 - 0.3*X2 + 0.8*X3 + 0.1*(X4**2) - 0.15*X1*X4\n",
        "    p = 1 / (1 + np.exp(-logits))\n",
        "    y = (np.random.rand(n) < p).astype(int)\n",
        "    # Inject missingness and a categorical variable\n",
        "    cat = np.random.choice([\"A\",\"B\",\"C\"], size=n, p=[0.5,0.3,0.2])\n",
        "    df = pd.DataFrame({\n",
        "        'feature_1': X1,\n",
        "        'feature_2': X2,\n",
        "        'feature_3_binary': X3,\n",
        "        'feature_4': X4,\n",
        "        'category': cat,\n",
        "        'target': y,\n",
        "    })\n",
        "    # Random missing values\n",
        "    miss_idx = np.random.choice(df.index, size=80, replace=False)\n",
        "    df.loc[miss_idx, 'feature_2'] = np.nan\n",
        "    df.loc[np.random.choice(df.index, 60, replace=False), 'category'] = np.nan\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Check basic structure, missingness, distributions, and correlations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df.shape)\n",
        "display(df.dtypes)\n",
        "display(df.describe(include='all'))\n",
        "\n",
        "# Missingness by column\n",
        "missing = df.isna().mean().sort_values(ascending=False)\n",
        "display(missing)\n",
        "\n",
        "# Simple missingness heatmap (binary mask)\n",
        "plt.figure()\n",
        "plt.imshow(df.isna(), aspect='auto', interpolation='nearest')\n",
        "plt.title('Missingness Heatmap (1=NaN)')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "plt.tight_layout()\n",
        "plt.savefig(ARTIFACTS / 'missingness_heatmap.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Histograms for numeric columns\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in num_cols:\n",
        "    plt.figure()\n",
        "    df[col].hist(bins=30)\n",
        "    plt.title(f'Distribution: {col}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ARTIFACTS / f'hist_{col}.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# Correlation heatmap (numeric only)\n",
        "corr = df[num_cols].corr(numeric_only=True)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(corr, interpolation='nearest')\n",
        "plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
        "plt.yticks(range(len(num_cols)), num_cols)\n",
        "plt.title('Correlation (numeric)')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.savefig(ARTIFACTS / 'correlation_heatmap.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cleaning & Preprocessing\n",
        "\n",
        "- Separate features/target\n",
        "- Split train/test early to avoid leakage\n",
        "- Impute missing values\n",
        "- One-hot encode categoricals\n",
        "- Standardize numeric features (as needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "TARGET = 'target' if 'target' in df.columns else df.columns[-1]\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET].astype(int) if df[TARGET].dropna().isin([0,1]).all() else df[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y if set(pd.unique(y)) <= {0,1} else None,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, num_features),\n",
        "        (\"cat\", categorical_transformer, cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "num_features, cat_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modeling (Leakage-Aware Pipelines)\n",
        "\n",
        "Build pipelines that include the preprocessor **inside** the cross-validation to avoid leakage.\n",
        "Try a few baseline models and evaluate with robust metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "is_binary = set(pd.unique(y_train.dropna())) <= {0,1}\n",
        "\n",
        "models = {\n",
        "    \"log_reg\": LogisticRegression(max_iter=200, n_jobs=None, random_state=SEED),\n",
        "    \"rf\": RandomForestClassifier(n_estimators=300, random_state=SEED),\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED) if is_binary else None\n",
        "scoring = 'roc_auc' if is_binary else 'r2'\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    pipe = Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", model)])\n",
        "    if cv is not None:\n",
        "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=scoring)\n",
        "        results[name] = {\n",
        "            \"cv_scores\": scores,\n",
        "            \"mean\": np.mean(scores),\n",
        "            \"std\": np.std(scores),\n",
        "        }\n",
        "    else:\n",
        "        # Fallback (e.g., regression case) using simple split\n",
        "        pipe.fit(X_train, y_train)\n",
        "        pred = pipe.predict(X_test)\n",
        "        results[name] = {\"test_score\": float(np.corrcoef(pred, y_test)[0,1])}\n",
        "\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Final Fit & Evaluation\n",
        "\n",
        "Choose the best model (by mean CV score), fit on the full training set, and evaluate\n",
        "on the held-out test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick best by mean CV score\n",
        "best_name = max(\n",
        "    (k for k in results if 'mean' in results[k]),\n",
        "    key=lambda k: results[k]['mean']\n",
        ")\n",
        "best_model = models[best_name]\n",
        "final_pipe = Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", best_model)])\n",
        "final_pipe.fit(X_train, y_train)\n",
        "\n",
        "if is_binary:\n",
        "    proba = final_pipe.predict_proba(X_test)[:,1]\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    print(\"Test ROC-AUC:\", roc_auc_score(y_test, proba))\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, pred))\n",
        "    print(\"Test F1:\", f1_score(y_test, pred))\n",
        "else:\n",
        "    pred = final_pipe.predict(X_test)\n",
        "    print(\"Test corr(pred, y):\", np.corrcoef(pred, y_test)[0,1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Uncertainty via Bootstrap Confidence Intervals\n",
        "\n",
        "Use bootstrap resampling to estimate a confidence interval for the chosen metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_ci(metric_fn, y_true, y_score, n_boot=1000, alpha=0.05, seed=SEED):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    scores = []\n",
        "    n = len(y_true)\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, n)\n",
        "        scores.append(metric_fn(y_true[idx], y_score[idx]))\n",
        "    scores = np.asarray(scores)\n",
        "    low = np.quantile(scores, alpha/2)\n",
        "    high = np.quantile(scores, 1 - alpha/2)\n",
        "    return float(np.mean(scores)), float(low), float(high)\n",
        "\n",
        "if is_binary:\n",
        "    proba = final_pipe.predict_proba(X_test)[:,1]\n",
        "    y_true = y_test.to_numpy()\n",
        "    auc_mean, auc_lo, auc_hi = bootstrap_ci(roc_auc_score, y_true, proba)\n",
        "    print(f\"Bootstrap ROC-AUC mean={auc_mean:.3f} CI=({auc_lo:.3f}, {auc_hi:.3f})\")\n",
        "else:\n",
        "    # Example for regression: correlation CI\n",
        "    pred = final_pipe.predict(X_test)\n",
        "    def corr_metric(y, yhat):\n",
        "        return np.corrcoef(y, yhat)[0,1]\n",
        "    m, lo, hi = bootstrap_ci(corr_metric, y_test.to_numpy(), pred)\n",
        "    print(f\"Bootstrap corr mean={m:.3f} CI=({lo:.3f}, {hi:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Explainability: Permutation Importance & Partial Dependence\n",
        "\n",
        "Permutation importance perturbs each feature and measures degradation in the metric.\n",
        "Partial dependence approximates the average effect of a feature on predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "\n",
        "if is_binary:\n",
        "    scoring_fn = lambda model, X, y: roc_auc_score(y, model.predict_proba(X)[:,1])\n",
        "else:\n",
        "    scoring_fn = lambda model, X, y: np.corrcoef(model.predict(X), y)[0,1]\n",
        "\n",
        "# Permutation importance (on test set)\n",
        "r = permutation_importance(final_pipe, X_test, y_test, n_repeats=10, random_state=SEED)\n",
        "\n",
        "# Extract feature names post-transform\n",
        "final_pipe.fit(X_train, y_train)\n",
        "pre = final_pipe.named_steps['preprocess']\n",
        "ohe = pre.named_transformers_['cat'].named_steps['onehot'] if len(pre.transformers_)>1 and 'cat' in dict(pre.transformers_).keys() else None\n",
        "num_names = num_features\n",
        "cat_names = ohe.get_feature_names_out(cat_features).tolist() if ohe is not None and len(cat_features)>0 else []\n",
        "feat_names = num_names + cat_names\n",
        "\n",
        "importances = pd.Series(r.importances_mean, index=feat_names).sort_values(ascending=False)\n",
        "display(importances.head(15))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "importances.head(15).iloc[::-1].plot(kind='barh')\n",
        "plt.title('Permutation Importance (top 15)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(ARTIFACTS / 'permutation_importance_top15.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Partial Dependence for top numeric features (if available)\n",
        "top_numeric = [f for f in importances.index if f in num_names][:2]\n",
        "if len(top_numeric) > 0:\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    PartialDependenceDisplay.from_estimator(final_pipe, X_test, features=top_numeric[:2])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ARTIFACTS / 'partial_dependence.png', dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric features available for partial dependence plots.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Statistical Inference (Hypothesis Testing & Effect Sizes)\n",
        "\n",
        "Example: compare means between two groups using a t-test, and compute Cohen's d.\n",
        "Adjust feature/target names to match your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Example grouping by a (possibly encoded) categorical feature.\n",
        "group_feature = 'category' if 'category' in df.columns else None\n",
        "numeric_feature = 'feature_2' if 'feature_2' in df.columns else None\n",
        "\n",
        "if group_feature and numeric_feature:\n",
        "    subset = df[[group_feature, numeric_feature]].dropna()\n",
        "    groups = subset[group_feature].dropna().unique()\n",
        "    if len(groups) >= 2:\n",
        "        g1 = subset[subset[group_feature] == groups[0]][numeric_feature]\n",
        "        g2 = subset[subset[group_feature] == groups[1]][numeric_feature]\n",
        "        t, p = stats.ttest_ind(g1, g2, equal_var=False)\n",
        "        # Cohen's d\n",
        "        def cohens_d(a, b):\n",
        "            na, nb = len(a), len(b)\n",
        "            sa, sb = np.var(a, ddof=1), np.var(b, ddof=1)\n",
        "            s_pool = (((na-1)*sa + (nb-1)*sb) / (na+nb-2)) ** 0.5\n",
        "            return (np.mean(a) - np.mean(b)) / s_pool\n",
        "        d = cohens_d(g1.values, g2.values)\n",
        "        print(f\"t={t:.3f} p={p:.3g} Cohen's d={d:.3f}\")\n",
        "    else:\n",
        "        print(\"Need at least two groups for t-test.\")\n",
        "else:\n",
        "    print(\"Skipping t-test: adjust 'group_feature' and 'numeric_feature' for your dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Artifacts & Environment\n",
        "\n",
        "Export model, figures, metrics, and the Python environment for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save final model\n",
        "MODEL_PATH = ARTIFACTS / 'final_model.joblib'\n",
        "joblib.dump(final_pipe, MODEL_PATH)\n",
        "print(f\"Saved model to {MODEL_PATH}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics = {}\n",
        "if is_binary:\n",
        "    proba = final_pipe.predict_proba(X_test)[:,1]\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    metrics = {\n",
        "        'roc_auc': float(roc_auc_score(y_test, proba)),\n",
        "        'accuracy': float(accuracy_score(y_test, pred)),\n",
        "        'f1': float(f1_score(y_test, pred)),\n",
        "    }\n",
        "else:\n",
        "    pred = final_pipe.predict(X_test)\n",
        "    metrics = {'corr': float(np.corrcoef(pred, y_test)[0,1])}\n",
        "\n",
        "with open(ARTIFACTS / 'metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(\"Saved metrics.json\")\n",
        "\n",
        "# Freeze environment\n",
        "ENV_FILE = ARTIFACTS / 'environment_freeze.txt'\n",
        "try:\n",
        "    import subprocess\n",
        "    out = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'], text=True)\n",
        "    ENV_FILE.write_text(out)\n",
        "    print(f\"Saved environment to {ENV_FILE}\")\n",
        "except Exception as e:\n",
        "    print(\"Could not freeze environment:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Next Steps\n",
        "\n",
        "- Consider a formal experiment plan and preregistration.\n",
        "- Add domain-specific feature engineering and priors.\n",
        "- Evaluate model fairness and sensitivity analyses.\n",
        "- Containerize the environment (e.g., Docker) for full portability.\n",
        "- Promote this notebook to a script + CLI for batch/CI runs.\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "date": "2025-10-20T16:29:31.382588Z",
        "name": "Generated by ChatGPT"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
